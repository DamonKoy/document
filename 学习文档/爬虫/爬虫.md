## 爬虫

## robots.txt协议：

- 君子协议。规定了网站中哪些数据可以被爬取，哪些数据不可以被爬取。

- 查看方法：在网页后面加上robots.txt，如链接：https://www.baidu.com/robots.txt  http://www.dianping.com/robots.txt

  ![image-20201127141908940](https://raw.githubusercontent.com/DamonKoy/document/dev/images/baidu-robots.png)

![image-20201127144054985](https://raw.githubusercontent.com/DamonKoy/document/dev/images/dingping-robots.png)

### http协议

- 服务器和客户端进行数据库交互的一种形式

### 常用请求头信息

- User-Agent：请求载体的身份标识
- Connection：请求完毕后，是断开连接还是保持连接

### 常用响应头信息

- Content-Type：服务器端响应回客户端的数据类型

### https协议

- 安全的超文本传输协议

### 加密方式

- 对称秘钥加密

  ![image-20201127153948561](https://raw.githubusercontent.com/DamonKoy/document/dev/images/对称秘钥加密.png)

- 非对称秘钥加密

  ![image-20201127153845677](https://raw.githubusercontent.com/DamonKoy/document/dev/images/非对称秘钥加密.png)

- 证书秘钥加密

  ![image-20201127153830890](https://raw.githubusercontent.com/DamonKoy/document/dev/images/%E8%AF%81%E4%B9%A6%E7%A7%98%E9%92%A5%E5%8A%A0%E5%AF%86.png)



### Requests 模块

#### urllib 模块（比较古老）

#### request模块

- python中原生的一款基于网络请求的模块，功能非常强大，简单便捷，效率极高。

- 作用：模拟浏览器发送请求

- 使用：编码流程

  - 指定url

  - 发起请求

  - 获取响应数据

  - 持久化存储

    

